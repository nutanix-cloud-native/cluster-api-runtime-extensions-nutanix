# Copyright 2024 Nutanix. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

on:
  workflow_call:
    inputs:
      provider:
        description: Infrastructure provider to run e2e tests with
        type: string
        required: true
      skip:
        description: e2e tests to skip
        type: string
      focus:
        description: e2e tests to focus
        type: string
      runs-on:
        description: The runner to run the e2e tests on
        type: string
        required: true
      kubernetes-version:
        description: The version of Kubernetes to test with
        type: string
        required: true
      base-os:
        description: The OS image to use for the machine template
        type: string
        required: false

jobs:
  e2e-test:
    runs-on: ${{ inputs.runs-on }}
    permissions:
      contents: read
      checks: write
    steps:
      - name: Check out code
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
      - name: Install devbox
        uses: jetify-com/devbox-install-action@v0.14.0
        with:
          enable-cache: ${{ inputs.runs-on != 'self-hosted-ncn-dind' }}

      - name: Export go version and mod cache path
        id: export-go-version-and-mod-cache-path
        run: |
          echo go-version="$(devbox run -- go version | cut -d ' ' -f 3)" >>"${GITHUB_OUTPUT}"
          echo mod-cache-path="$(devbox run -- go env GOMODCACHE)" >>"${GITHUB_OUTPUT}"

      - name: Go cache
        uses: actions/cache@v4
        with:
          path: |
            ${{ steps.export-go-version-and-mod-cache-path.outputs.mod-cache-path }}
          key: ${{ runner.os }}-${{ steps.export-go-version-and-mod-cache-path.outputs.go-version }}-${{ hashFiles('**/go.sum') }}
          restore-keys: |
            ${{ runner.os }}-${{ steps.export-go-version-and-mod-cache-path.outputs.go-version }}-

      # The default disk size of Github hosted runners is ~14GB, this is not enough to run the e2e tests.
      # Cleanup the disk, see upstream discussion https://github.com/actions/runner-images/issues/2840.
      - name: Cleanup Disk Space
        if: inputs.runs-on != 'self-hosted-ncn-dind'
        run: |
          echo "Before removing files:"
          df -h
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf "/usr/local/share/boost"
          sudo rm -rf "${AGENT_TOOLSDIRECTORY}"
          echo "After removing files:"
          df -h

      - name: Run e2e tests
        run: devbox run -- make e2e-test E2E_LABEL='provider:${{ inputs.provider }}' E2E_SKIP='${{ inputs.skip }}' E2E_FOCUS='${{ inputs.focus }}' E2E_VERBOSE=true E2E_SKIP_CLEANUP=true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          DOCKER_HUB_USERNAME: ${{ secrets.DOCKER_HUB_USERNAME }}
          DOCKER_HUB_PASSWORD: ${{ secrets.DOCKER_HUB_PASSWORD }}
          NUTANIX_ENDPOINT: ${{ secrets.NUTANIX_ENDPOINT }}
          NUTANIX_USER: ${{ secrets.NUTANIX_USER }}
          NUTANIX_PASSWORD: ${{ secrets.NUTANIX_PASSWORD }}
          NUTANIX_PORT: ${{ vars.NUTANIX_PORT }}
          NUTANIX_INSECURE: false
          NUTANIX_PRISM_ELEMENT_CLUSTER_NAME: ${{ vars.NUTANIX_PRISM_ELEMENT_CLUSTER_NAME }}
          NUTANIX_SUBNET_NAME: ${{ vars.NUTANIX_SUBNET_NAME }}
          NUTANIX_STORAGE_CONTAINER_NAME: ${{ vars.NUTANIX_STORAGE_CONTAINER_NAME }}
          NUTANIX_MACHINE_TEMPLATE_BASE_OS: ${{ inputs.base-os }}
          KUBERNETES_VERSION_NUTANIX: ${{ inputs.kubernetes-version }}
          KINDEST_IMAGE_TAG: ${{ inputs.kubernetes-version }}
          E2E_KUBERNETES_VERSION: ${{ inputs.kubernetes-version }}

      - name: Collect bootstrap cluster pod logs on failure
        if: failure()
        run: |
          set -x  # Enable debug mode to see all commands
          echo "===== Starting log collection ====="
          mkdir -p bootstrap-pod-logs

          # Check if Kind cluster exists first
          echo "Checking for Kind clusters..."
          devbox run -- kind get clusters > bootstrap-pod-logs/kind-clusters.txt 2>&1 || true
          cat bootstrap-pod-logs/kind-clusters.txt

          # Get kubeconfig from Kind cluster (name: caren-e2e from test/e2e/config/caren.yaml)
          echo "Getting kubeconfig from Kind cluster 'caren-e2e'..."
          if devbox run -- kind get kubeconfig --name caren-e2e > bootstrap-pod-logs/kubeconfig.yaml 2>&1; then
            echo "✓ Successfully retrieved kubeconfig from Kind cluster"

            # Use absolute path for KUBECONFIG
            export KUBECONFIG="${PWD}/bootstrap-pod-logs/kubeconfig.yaml"
            echo "✓ KUBECONFIG set to: $KUBECONFIG"

            # Verify kubectl works
            echo "Verifying kubectl connectivity..."
            if kubectl cluster-info &> bootstrap-pod-logs/cluster-info.txt; then
              echo "✓ kubectl can connect to cluster"
              cat bootstrap-pod-logs/cluster-info.txt
            else
              echo "✗ kubectl cannot connect to cluster"
              cat bootstrap-pod-logs/cluster-info.txt
              exit 0  # Exit gracefully, don't fail the workflow
            fi

            # Get all pods overview
            echo "Getting all pods..."
            kubectl get pods -A -o wide > bootstrap-pod-logs/all-pods.txt 2>&1 || echo "Failed to get pods" > bootstrap-pod-logs/all-pods.txt
            echo "✓ Saved all-pods.txt"

            # Get all namespaces
            kubectl get namespaces > bootstrap-pod-logs/namespaces.txt 2>&1 || true
            echo "✓ Saved namespaces.txt"

            # Get CAREN controller logs (main focus - this is what you need for konnector-agent debugging)
            echo "Collecting CAREN controller logs..."
            kubectl logs -n caren-system -l app.kubernetes.io/name=cluster-api-runtime-extensions-nutanix --all-containers=true --tail=1000 > bootstrap-pod-logs/caren-controller.log 2>&1 || echo "Failed to get CAREN logs" > bootstrap-pod-logs/caren-controller.log
            echo "✓ Saved caren-controller.log ($(wc -l < bootstrap-pod-logs/caren-controller.log) lines)"

            # Get CAREN controller pod descriptions
            kubectl describe pods -n caren-system -l app.kubernetes.io/name=cluster-api-runtime-extensions-nutanix > bootstrap-pod-logs/caren-pods-describe.txt 2>&1 || true
            echo "✓ Saved caren-pods-describe.txt"

            # Get all CAPI provider logs
            echo "Collecting CAPI provider logs..."
            for ns in capi-system capi-kubeadm-bootstrap-system capi-kubeadm-control-plane-system capd-system capn-system capa-system caaph-system; do
              if kubectl get namespace "$ns" >/dev/null 2>&1; then
                echo "  ✓ Collecting logs from namespace: $ns"
                mkdir -p "bootstrap-pod-logs/$ns"
                kubectl get pods -n "$ns" -o wide > "bootstrap-pod-logs/$ns/pods.txt" 2>&1 || true
                for pod in $(kubectl get pods -n "$ns" -o name 2>/dev/null); do
                  pod_name=$(basename "$pod")
                  kubectl logs -n "$ns" "$pod_name" --all-containers=true --tail=500 > "bootstrap-pod-logs/$ns/${pod_name}.log" 2>&1 || true
                done
              fi
            done

            # Get HelmReleaseProxy and HelmChartProxy resources (critical for konnector-agent debugging)
            echo "Collecting Helm addon resources..."
            kubectl get helmreleaseproxies -A -o yaml > bootstrap-pod-logs/helmreleaseproxies.yaml 2>&1 || true
            kubectl get helmchartproxies -A -o yaml > bootstrap-pod-logs/helmchartproxies.yaml 2>&1 || true
            echo "✓ Saved Helm resources"

            # Get all events (helpful for debugging)
            echo "Collecting cluster events..."
            kubectl get events -A --sort-by='.lastTimestamp' > bootstrap-pod-logs/events.txt 2>&1 || true
            echo "✓ Saved events.txt"
            
            # Get all clusters
            echo "Getting list of workload clusters..."
            kubectl get clusters -A > bootstrap-pod-logs/clusters.txt 2>&1 || true
            
            # Collect logs from workload cluster (where konnector-agent actually runs)
            echo "===== Collecting logs from WORKLOAD CLUSTER ====="
            mkdir -p bootstrap-pod-logs/workload-cluster
            
            # Find the workload cluster name (starts with "quick-start")
            WORKLOAD_CLUSTER=$(kubectl get clusters -A -o jsonpath='{.items[?(@.metadata.name contains "quick-start")].metadata.name}' 2>/dev/null | head -n 1)
            WORKLOAD_NAMESPACE=$(kubectl get clusters -A -o jsonpath='{.items[?(@.metadata.name contains "quick-start")].metadata.namespace}' 2>/dev/null | head -n 1)
            
            if [[ -n "$WORKLOAD_CLUSTER" && -n "$WORKLOAD_NAMESPACE" ]]; then
              echo "Found workload cluster: $WORKLOAD_CLUSTER in namespace $WORKLOAD_NAMESPACE"
              
              # Get workload cluster kubeconfig from secret
              echo "Retrieving workload cluster kubeconfig..."
              if kubectl get secret -n "$WORKLOAD_NAMESPACE" "${WORKLOAD_CLUSTER}-kubeconfig" -o jsonpath='{.data.value}' 2>/dev/null | base64 -d > bootstrap-pod-logs/workload-cluster/kubeconfig.yaml; then
                export WORKLOAD_KUBECONFIG="${PWD}/bootstrap-pod-logs/workload-cluster/kubeconfig.yaml"
                echo "✓ Retrieved workload cluster kubeconfig"
                
                # Test connectivity
                if kubectl --kubeconfig="$WORKLOAD_KUBECONFIG" cluster-info &> bootstrap-pod-logs/workload-cluster/cluster-info.txt; then
                  echo "✓ Can connect to workload cluster"
                  
                  # Get all pods in ntnx-system namespace
                  echo "Getting pods in ntnx-system namespace..."
                  kubectl --kubeconfig="$WORKLOAD_KUBECONFIG" get pods -n ntnx-system -o wide > bootstrap-pod-logs/workload-cluster/ntnx-system-pods.txt 2>&1 || true
                  
                  # Get konnector-agent pod descriptions (THIS IS WHAT YOU WANT!)
                  echo "Getting konnector-agent pod descriptions..."
                  kubectl --kubeconfig="$WORKLOAD_KUBECONFIG" describe pods -n ntnx-system -l app.kubernetes.io/name=konnector-agent > bootstrap-pod-logs/workload-cluster/konnector-agent-describe.txt 2>&1 || true
                  
                  # Get konnector-agent pod logs
                  echo "Getting konnector-agent pod logs..."
                  for pod in $(kubectl --kubeconfig="$WORKLOAD_KUBECONFIG" get pods -n ntnx-system -l app.kubernetes.io/name=konnector-agent -o name 2>/dev/null); do
                    pod_name=$(basename "$pod")
                    echo "  Getting logs for $pod_name..."
                    kubectl --kubeconfig="$WORKLOAD_KUBECONFIG" logs -n ntnx-system "$pod_name" --all-containers=true --tail=1000 > "bootstrap-pod-logs/workload-cluster/${pod_name}.log" 2>&1 || true
                    kubectl --kubeconfig="$WORKLOAD_KUBECONFIG" logs -n ntnx-system "$pod_name" --all-containers=true --previous --tail=500 > "bootstrap-pod-logs/workload-cluster/${pod_name}-previous.log" 2>&1 || true
                  done
                  
                  # Get hook pod descriptions and logs (hook-preinstall is what fails)
                  echo "Getting hook pod information..."
                  kubectl --kubeconfig="$WORKLOAD_KUBECONFIG" get pods -n ntnx-system | grep hook > bootstrap-pod-logs/workload-cluster/hook-pods.txt 2>&1 || true
                  
                  for hook_pod in hook-preinstall hook-postinstall; do
                    if kubectl --kubeconfig="$WORKLOAD_KUBECONFIG" get pod -n ntnx-system "$hook_pod" &>/dev/null; then
                      echo "  Found $hook_pod, collecting info..."
                      kubectl --kubeconfig="$WORKLOAD_KUBECONFIG" describe pod -n ntnx-system "$hook_pod" > "bootstrap-pod-logs/workload-cluster/${hook_pod}-describe.txt" 2>&1 || true
                      kubectl --kubeconfig="$WORKLOAD_KUBECONFIG" logs -n ntnx-system "$hook_pod" --all-containers=true --tail=1000 > "bootstrap-pod-logs/workload-cluster/${hook_pod}.log" 2>&1 || true
                    fi
                  done
                  
                  # Get konnector-agent secret info (without credentials)
                  echo "Getting konnector-agent secret info..."
                  if kubectl --kubeconfig="$WORKLOAD_KUBECONFIG" get secret -n ntnx-system konnector-agent &>/dev/null; then
                    kubectl --kubeconfig="$WORKLOAD_KUBECONFIG" get secret -n ntnx-system konnector-agent -o yaml | grep -v "password\|credential" > bootstrap-pod-logs/workload-cluster/konnector-agent-secret.yaml 2>&1 || true
                    echo "Secret keys present:" > bootstrap-pod-logs/workload-cluster/konnector-agent-secret-keys.txt
                    kubectl --kubeconfig="$WORKLOAD_KUBECONFIG" get secret -n ntnx-system konnector-agent -o jsonpath='{.data}' | grep -o '"[^"]*":' | tr -d '":' >> bootstrap-pod-logs/workload-cluster/konnector-agent-secret-keys.txt 2>&1 || true
                  fi
                  
                  # Get workload cluster events
                  echo "Getting workload cluster events..."
                  kubectl --kubeconfig="$WORKLOAD_KUBECONFIG" get events -n ntnx-system --sort-by='.lastTimestamp' > bootstrap-pod-logs/workload-cluster/events.txt 2>&1 || true
                  
                  # Get all namespaces in workload cluster
                  kubectl --kubeconfig="$WORKLOAD_KUBECONFIG" get namespaces > bootstrap-pod-logs/workload-cluster/namespaces.txt 2>&1 || true
                  
                  echo "✓ Workload cluster logs collected successfully"
                else
                  echo "✗ Cannot connect to workload cluster"
                  cat bootstrap-pod-logs/workload-cluster/cluster-info.txt
                fi
              else
                echo "✗ Failed to retrieve workload cluster kubeconfig"
              fi
            else
              echo "No workload cluster found (this is normal if cluster creation failed early)"
            fi
            
            echo "===== ✓ Log collection completed successfully ====="
            ls -lh bootstrap-pod-logs/
          else
            echo "===== ✗ Failed to get kubeconfig ====="
            cat bootstrap-pod-logs/kubeconfig.yaml || echo "No kubeconfig file"
            echo "Checking Docker containers..."
            docker ps -a > bootstrap-pod-logs/docker-ps.txt 2>&1 || true
          fi

      - name: Upload bootstrap pod logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: bootstrap-pod-logs-${{ inputs.provider }}-${{ inputs.kubernetes-version }}
          path: bootstrap-pod-logs/
          if-no-files-found: warn

      - name: Cleanup Kind cluster after log collection
        if: always()
        run: |
          echo "Cleaning up Kind cluster..."
          devbox run -- kind get clusters || true
          devbox run -- kind delete cluster --name caren-e2e || true
          echo "Cleanup completed"

      - if: success() || failure() # always run even if the previous step fails
        name: Publish e2e test report
        uses: mikepenz/action-junit-report@v5
        with:
          report_paths: 'junit-e2e.xml'
          check_name: 'e2e test report'
          detailed_summary: true
          require_passed_tests: true
