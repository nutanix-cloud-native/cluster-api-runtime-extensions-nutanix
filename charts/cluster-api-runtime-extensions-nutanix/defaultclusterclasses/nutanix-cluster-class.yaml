apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  labels:
    cluster.x-k8s.io/provider: nutanix
  name: nutanix-quick-start-kcfg-0
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: external
            eviction-hard: nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<15%,memory.available<100Mi,imagefs.inodesFree<10%
            tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
      postKubeadmCommands:
      - echo "after kubeadm call" > /var/log/postkubeadm.log
      preKubeadmCommands:
      - echo "before kubeadm call" > /var/log/prekubeadm.log
      - hostnamectl set-hostname "{{ ds.meta_data.hostname }}"
      verbosity: 10
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: ClusterClass
metadata:
  labels:
    cluster.x-k8s.io/provider: nutanix
  name: nutanix-quick-start
spec:
  controlPlane:
    machineHealthCheck:
      maxUnhealthy: 40%
      nodeStartupTimeout: 10m
      unhealthyConditions:
      - status: "False"
        timeout: 300s
        type: Ready
      - status: Unknown
        timeout: 300s
        type: Ready
      - status: "True"
        timeout: 300s
        type: MemoryPressure
      - status: "True"
        timeout: 300s
        type: DiskPressure
      - status: "True"
        timeout: 300s
        type: PIDPressure
      - status: "True"
        timeout: 300s
        type: NetworkUnavailable
    machineInfrastructure:
      ref:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: NutanixMachineTemplate
        name: nutanix-quick-start-cp-nmt
    ref:
      apiVersion: controlplane.cluster.x-k8s.io/v1beta1
      kind: KubeadmControlPlaneTemplate
      name: nutanix-quick-start-kcpt
  infrastructure:
    ref:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: NutanixClusterTemplate
      name: nutanix-quick-start-nct
  patches:
  - external:
      discoverVariablesExtension: nutanixclusterconfigvars.cluster-api-runtime-extensions-nutanix
      generateExtension: nutanixclusterconfigpatch.cluster-api-runtime-extensions-nutanix
    name: cluster-config
  - external:
      discoverVariablesExtension: nutanixworkerconfigvars.cluster-api-runtime-extensions-nutanix
      generateExtension: nutanixworkerconfigpatch.cluster-api-runtime-extensions-nutanix
    name: worker-config
  workers:
    machineDeployments:
    - class: default-worker
      machineHealthCheck:
        maxUnhealthy: 40%
        nodeStartupTimeout: 10m
        unhealthyConditions:
        - status: "False"
          timeout: 300s
          type: Ready
        - status: Unknown
          timeout: 300s
          type: Ready
        - status: "True"
          timeout: 300s
          type: MemoryPressure
        - status: "True"
          timeout: 300s
          type: DiskPressure
        - status: "True"
          timeout: 300s
          type: PIDPressure
        - status: "True"
          timeout: 300s
          type: NetworkUnavailable
      template:
        bootstrap:
          ref:
            apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
            kind: KubeadmConfigTemplate
            name: nutanix-quick-start-kcfg-0
        infrastructure:
          ref:
            apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
            kind: NutanixMachineTemplate
            name: nutanix-quick-start-md-nmt
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlaneTemplate
metadata:
  labels:
    cluster.x-k8s.io/provider: nutanix
  name: nutanix-quick-start-kcpt
spec:
  template:
    spec:
      kubeadmConfigSpec:
        clusterConfiguration:
          apiServer:
            extraArgs:
              cloud-provider: external
              tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          controllerManager:
            extraArgs:
              cloud-provider: external
              tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          scheduler:
            extraArgs:
              tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
        files: []
        initConfiguration:
          nodeRegistration:
            kubeletExtraArgs:
              cloud-provider: external
              eviction-hard: nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<15%,memory.available<100Mi,imagefs.inodesFree<10%
              tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
        joinConfiguration:
          nodeRegistration:
            kubeletExtraArgs:
              cloud-provider: external
              eviction-hard: nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<15%,memory.available<100Mi,imagefs.inodesFree<10%
              tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
        postKubeadmCommands:
        - echo export KUBECONFIG=/etc/kubernetes/admin.conf >> /root/.bashrc
        - echo "after kubeadm call" > /var/log/postkubeadm.log
        preKubeadmCommands:
        - echo "before kubeadm call" > /var/log/prekubeadm.log
        - hostnamectl set-hostname "{{ ds.meta_data.hostname }}"
        - echo "::1         ipv6-localhost ipv6-loopback" >/etc/hosts
        - echo "127.0.0.1   localhost" >>/etc/hosts
        - echo "127.0.0.1   {{ ds.meta_data.hostname }}" >> /etc/hosts
        useExperimentalRetryJoin: true
        verbosity: 10
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: NutanixClusterTemplate
metadata:
  labels:
    cluster.x-k8s.io/provider: nutanix
  name: nutanix-quick-start-nct
spec:
  template:
    spec:
      controlPlaneEndpoint:
        host: PLACEHOLDER
        port: 6443
      failureDomains: []
      prismCentral:
        address: PLACEHOLDER
        credentialRef:
          kind: Secret
          name: PLACEHOLDER
          namespace: default
        port: 9440
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: NutanixMachineTemplate
metadata:
  labels:
    cluster.x-k8s.io/provider: nutanix
  name: nutanix-quick-start-cp-nmt
spec:
  template:
    spec:
      bootType: legacy
      cluster:
        name: ""
        type: name
      image:
        name: ""
        type: name
      memorySize: 4Gi
      subnet:
      - name: ""
        type: name
      systemDiskSize: 40Gi
      vcpuSockets: 2
      vcpusPerSocket: 1
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: NutanixMachineTemplate
metadata:
  labels:
    cluster.x-k8s.io/provider: nutanix
  name: nutanix-quick-start-md-nmt
spec:
  template:
    spec:
      bootType: legacy
      cluster:
        name: ""
        type: name
      image:
        name: ""
        type: name
      memorySize: 4Gi
      subnet:
      - name: ""
        type: name
      systemDiskSize: 40Gi
      vcpuSockets: 2
      vcpusPerSocket: 1
